{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a66e93",
   "metadata": {},
   "source": [
    "# Analysing and Tracking Environment topic on Reddit\n",
    "Su Myat Noe Yee (s3913797)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113f6fe6",
   "metadata": {},
   "source": [
    "### Loading required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87cc68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import string\n",
    "import praw\n",
    "import nltk\n",
    "import codecs\n",
    "import re\n",
    "import datetime\n",
    "import nltk\n",
    "import matplotlib.pyplot as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from colorama import Fore, Back, Style\n",
    "from gensim.corpora import Dictionary\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import cross_validate\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "import math\n",
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e1a55d",
   "metadata": {},
   "source": [
    "Automatically reload the client information if there are changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfa6e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport redditClient\n",
    "%aimport RedditProcessing\n",
    "from redditClient import redditClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5293b3c",
   "metadata": {},
   "source": [
    "### Constructing Reddit client and subreddit name we interested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667e0754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Su_05\n"
     ]
    }
   ],
   "source": [
    "# Construct Reddit client and check it\n",
    "client = redditClient()\n",
    "print(client.user.me())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cce3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddit name we interested in getting the hot submissions\n",
    "sSubredditName = \"environment\"\n",
    "\n",
    "# Set the maximum number of hot submissions we want to retrieve\n",
    "hotLimit = 200\n",
    "\n",
    "# Get the subreddit object\n",
    "subreddit = client.subreddit(sSubredditName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7959a7",
   "metadata": {},
   "source": [
    "### Titles of Hot Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "394d969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "title_count = 0;\n",
    "\n",
    "# Specify the target date (August 20, 2023)\n",
    "target_date = datetime.datetime(2023, 8, 25)\n",
    "\n",
    "# Print out the titles of hot submissions up until August 20, 2023\n",
    "for submission in subreddit.hot(limit=hotLimit):\n",
    "    submission_date = datetime.datetime.fromtimestamp(submission.created_utc)\n",
    "    if submission_date.date() <= target_date.date():\n",
    "        print(submission.title)\n",
    "        title_count += 1\n",
    "\n",
    "print(title_count);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2758c0a7",
   "metadata": {},
   "source": [
    "### Users/ Authors Participation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d7965b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Sorin61': 17, 'Wagamaga': 17, 'pnewell': 15, 'misana123': 13, 'chrisdh79': 11, 'YoanB': 10, 'anutensil': 9, 'DukeOfGeek': 8, 'boppinmule': 7, 'lnfinity': 6, 'Fr1sk3r': 6, 'newnemo': 5, 'justin_quinnn': 5, 'Splenda': 5, 'DoremusJessup': 5, 'tta2013': 5, 'WhoIsJolyonWest': 5, 'mvea': 5, 'StoopSign': 4, 'silence7': 4, 'WilliamBlack97AI': 4, 'madazzahatter': 4, 'Huplescat22': 3, 'maki23': 3, 'sasht': 3, 'redcondurango': 3, 'maxwellhill': 3, 'AlphaXChance': 2, 'CrJ418': 2, 'FreedomsPower': 2, 'Maxcactus': 2, 'FERNnews': 2, 'Sariel007': 2, 'esporx': 2, 'Toadfinger': 2, 'Exastiken': 2, 'washingtonpost': 2, 'Ovaz1088': 2, 'ruellph': 2, 'Digimaverick': 2, 'thinkB4WeSpeak': 2, 'GoMx808-0': 2, 'morenewsat11': 2, 'SportsBetter': 2, 'AngelaMotorman': 2, 'holyfruits': 2, 'judeharte': 2, 'Dragonlance12': 2, 'HappySilveon': 2, 'ManiaforBeatles': 2, 'AdamCannon': 2, 'proctorberlin': 2, 'r4816': 2, 'pheonix200': 2, 'BringBackParis': 2, 'readerseven': 2, 'Logibenq': 1, 'ae314': 1, 'LiveScience_': 1, 'BlankVerse': 1, 'cnbc_official': 1, 'thisisinsider': 1, 'OregonTripleBeam': 1, 'Substantial_Cat_6001': 1, 'hernannadal': 1, 'Tomagotchu': 1, 'Tao_Dragon': 1, 'n1ght_w1ng08': 1, 'No_Bug1481': 1, 'LeMonde_en': 1, 'GrandpaChainz': 1, 'donutloop': 1, 'Dystopiaian': 1, 'Minimum-Net-7506': 1, 'Saltedline': 1, 'Finance_mechanism': 1, 'michaelrch': 1, 'Gopu_17': 1, 'KushtySmash': 1, 'You_lil_gumper': 1, 'coolbern': 1, 'Fun-Draft1612': 1, 'harsh2k5': 1, 'fungussa': 1, 'Frubanoid': 1, 'TrixoftheTrade': 1, 'popsci': 1, 'Yogurt789': 1, 'HorsesMeow': 1, 'Quirkie': 1, 'marketrent': 1, 'ZiaSoul': 1, 'TeeKu13': 1, 'Handicapreader': 1, 'ianbirmingham': 1, 'storming_heaven': 1, 'zsreport': 1, 'thaw4188': 1, 'spectaclecommodity': 1, 'crustose_lichen': 1, 'jonfla': 1, 'PhilDesenex': 1, 'snesdreams': 1, 'yash13': 1, 'chiquisea': 1, 'SwangyThang': 1, 'HTownWanderer': 1, 'nick313': 1, 'PoorIsTheNewSwag': 1, 'WasatchBackDarkSkies': 1, 'Tarterus1454': 1, 'turbo_dude': 1, 'night-mail': 1, 'madelinethespyNC': 1, 'jms1225': 1, 'Jariiari7': 1, 'B0ssc0': 1, 'LumpySpikes': 1, 'inthesetimesmag': 1, 'APnews': 1, 'LudovicoSpecs': 1, 'stockhackerDFW': 1, 'terios23132': 1, 'Mobalise_Anarchise': 1, 'Pessimist2020': 1, 'riougenkaku': 1, 'BelgianPolitics': 1, 'cam_man_can': 1, 'CarbonQuality': 1, 'cheechssoup': 1, 'Heagss': 1, 'Remedy_Review': 1, 'spsheridan': 1, 'chicknlil': 1, 'Miserable-Lizard': 1, 'Snukii': 1, 'radical_vegan': 1, 'Seronen': 1, 'bllshrfv': 1, 'crackulates': 1, 'SW_Green-Classifieds': 1, 'MilanMahata': 1, 'UniProcrastinator': 1, 'thatjoachim': 1, 'ChrissyKin_93': 1, 'Mighty_Zote': 1, 'Nick__________': 1, 'TusharJB007': 1, 'caj_gol': 1, 'XXmynameisNeganXX': 1, 'cryptoz': 1, 'expletivdeleted': 1, 'EightRoundsRapid': 1, 'ThugznKisses': 1, 'HistoryBuff97': 1, 'Elyzion-111': 1, 'PaintSniffer69': 1, 'Kalifornier': 1, 'd_tinker': 1, 'ILikeNeurons': 1, 'dookiea': 1, 'WinkleCream': 1, 'shallah': 1, 'natureboyldn': 1, 'veraknow': 1, 'MarzipanDefiant7586': 1, 'QuietCakeBionics': 1, 'drewiepoodle': 1, 'joez37': 1, 'YaarKhaa': 1, 'ChickenTeriyakiBoy1': 1, 'RomneysBainer': 1, 'ZoneRangerMC': 1, 'aelfredthegrape': 1, 'glenskin90': 1, 'Firm_Relative_7283': 1, 'hillarioushillary': 1, 'BreadTubeForever': 1, 'dont_tread_on_dc': 1, 'glasspee': 1, '_whatever-nevermind': 1, 'Supremetacoleader': 1, 'MichaelTen': 1, 'Daniel_Toben': 1, 'TheSurfShack': 1, 'papo96': 1, 'cyanocittaetprocyon': 1, 'stankmanly': 1, 'tottocotunio': 1, 'EustacheDaugerLives': 1, 'magenta_placenta': 1, 'Appropriate_Ant_4629': 1, 'widow2': 1, 'soft_azure': 1, 'CharyBrown': 1, 'RosetteNewcomb': 1, 'JeffBeauregard3': 1, 'progress18': 1, 'Gf1zzle': 1, 'ScipioA': 1})\n"
     ]
    }
   ],
   "source": [
    "lHotNames = []\n",
    "lTopNames = []\n",
    "\n",
    "# Obtain the hot submissions for this subreddit\n",
    "for submission in subreddit.hot(limit=hotLimit):\n",
    "    # Check if the submission has an author\n",
    "    if submission.author is not None:\n",
    "        # Append the author's username to the list\n",
    "        lHotNames.append(submission.author.name)\n",
    "\n",
    "# Obtain the top submissions for this subreddit\n",
    "for submission in subreddit.top(limit=hotLimit):\n",
    "    if submission.author is not None:\n",
    "        lTopNames.append(submission.author.name)\n",
    "        \n",
    "# Combine the lists together\n",
    "lHotNames.extend(lTopNames)\n",
    "# Count the number of times each user appears in the combined list\n",
    "hNameCount = Counter(lHotNames)\n",
    "print(hNameCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8cbf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 authors and their submission counts\n",
    "top_authors = hNameCount.most_common(20)\n",
    "top_author_names = [author for author, count in top_authors]\n",
    "top_author_counts = [count for author, count in top_authors]\n",
    "\n",
    "# Create a bar chart for the top 20 authors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_author_names, top_author_counts)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel('Authors')\n",
    "plt.ylabel('Number of Submissions')\n",
    "plt.title('Top 20 Authors with Highest Submission Counts')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83d7e20",
   "metadata": {},
   "source": [
    "### Top Commentors in Hot Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43be80f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lCommentAuthors = []\n",
    "\n",
    "for submission in subreddit.hot(limit=hotLimit):\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    for comment in submission.comments.list():\n",
    "        if comment.author is not None:\n",
    "            lCommentAuthors.append(comment.author.name)\n",
    "            \n",
    "# Count the number of times users are authors of the comments\n",
    "lCommentorCount = Counter(lCommentAuthors)\n",
    "print(lCommentorCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9bbead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 20 commentors and their submission counts\n",
    "top_commentors = lCommentorCount.most_common(20)\n",
    "top_commentors_names = [author for author, count in top_commentors]\n",
    "top_commentors_counts = [count for author, count in top_commentors]\n",
    "\n",
    "# Create a bar chart for the top 20 commentors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_commentors_names, top_commentors_counts)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel('Commentors')\n",
    "plt.ylabel('Number of Submissions')\n",
    "plt.title('Top 20 Commentors with Highest Submission Counts')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989cfdf8",
   "metadata": {},
   "source": [
    "### Exporting json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Fetch submissions from the specified subreddit\n",
    "submissions = []\n",
    "\n",
    "for submission in subreddit.hot(limit=hotLimit):\n",
    "        submission_data = {\n",
    "            \"title\": submission.title,\n",
    "            \"author\": submission.author.name if submission.author else None,\n",
    "            \"score\": submission.score,\n",
    "            \"comments\": [\n",
    "                {\n",
    "                    \"author\": comment.author.name if comment.author else None,\n",
    "                    \"created\": comment.created_utc,\n",
    "                    \"text\": comment.body,\n",
    "                }\n",
    "                for comment in submission.comments\n",
    "            ],\n",
    "            \"created\": submission.created_utc,\n",
    "        }\n",
    "        submissions.append(submission_data)\n",
    "\n",
    "# Create a dictionary to match the provided format\n",
    "data = {\"submissions\": submissions}\n",
    "\n",
    "# Save the data to a JSON file\n",
    "output_filename = \"environment.json\"\n",
    "with open(output_filename, \"w\") as json_file:\n",
    "    json.dump(data, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd14735",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "### Tokenisation removing stopwords and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b058d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processText(text, tokenizer, stemmer, stopwords):\n",
    "    \"\"\"\n",
    "    Perform tokenisation, normalisation (lower case and stemming) and stopword and twitter keyword removal.\n",
    "    @param text: reddit submission or comment text\n",
    "    @param tokenizer: tokeniser used.\n",
    "    @param stemmer: stemmer used.\n",
    "    @param stopwords: list of stopwords used\n",
    "    @returns: a list of processed tokens\n",
    "    \"\"\"\n",
    "    # Initialize counters\n",
    "    total_unfiltered_tokens = 0\n",
    "    total_filtered_tokens = 0\n",
    "\n",
    "    # Covert all to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenisation\n",
    "    lTokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Strip whitespaces before and after\n",
    "    lTokens = [token.strip() for token in lTokens]\n",
    "\n",
    "    # Remove stopwords, digits and username and non-ASCII words\n",
    "    processed_tokens = []\n",
    "    for tok in lTokens:\n",
    "        if tok not in stopwords and not tok.isdigit() and not tok.startswith('@') and tok.isascii():\n",
    "            processed_tokens.append(tok)\n",
    "            \n",
    "    # Return processed tokens\n",
    "    return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35de379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading json file and preparation for text pre-processing\n",
    "fJsonName = 'environment.json'\n",
    "\n",
    "# Number of most frequent terms to display\n",
    "freqNum = 50\n",
    "\n",
    "# Tweet tokeniser to use\n",
    "tweetTokeniser = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "# Preparing stopwords\n",
    "# Use the punctuation symbols defined in string.punctuation\n",
    "lPunct = list(string.punctuation)\n",
    "# Use stopwords from nltk and a few other specific terms\n",
    "lStopwords = nltk.corpus.stopwords.words('english') + lPunct + ['rt', 'via', 'u', '...', 'â€¦', '\"', \"'\", '`']\n",
    "\n",
    "# Porter stemmer for Stemming\n",
    "tweetStemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "# Term frequency counter\n",
    "termFreqCounter = Counter()\n",
    "\n",
    "# This will store the list of posts including title and comments(Combined processed tokens) we read from subreddit for LDA analysis\n",
    "lPosts_LDA = [] \n",
    "\n",
    "# Open json file and process it tweet by tweet\n",
    "with open(fJsonName, 'r') as f:\n",
    "    dSubmissions = json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b65f8",
   "metadata": {},
   "source": [
    "### Daily Count of Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_submission_dates = [] # Empty list to store the submission dates\n",
    " \n",
    "for submission in dSubmissions['submissions']:\n",
    "    submission_date = datetime.datetime.fromtimestamp(submission['created'])\n",
    "    count_submission_dates.append(submission_date)\n",
    "\n",
    "# Create a pandas DataFrame with submission dates\n",
    "df = pd.DataFrame({'date': count_submission_dates})\n",
    "\n",
    "# Convert the 'date' column to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Set the 'date' column as the index\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the data for daily and hourly counts\n",
    "daily_counts = df.resample('D').size()\n",
    "\n",
    "# Plot the time series of daily counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "daily_counts.plot()\n",
    "plt.title('Daily Count of Posts')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Posts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26d467",
   "metadata": {},
   "source": [
    "### Analyzing Term Frequency in Reddit Submissions Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17200b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in dSubmissions['submissions']:\n",
    "    submissionsTitle = submission.get('title', '') \n",
    "    # Tokenise, filter stopwords and get convert to lower case\n",
    "    lTokens = processText(text=submissionsTitle, tokenizer=tweetTokeniser, stemmer=tweetStemmer, stopwords=lStopwords)\n",
    "    # Update count\n",
    "    termFreqCounter.update(lTokens)\n",
    "    # All tokens of each text are combined for further topic modelling, LDA.\n",
    "    #lPosts.append(' '.join(lTokens)) \n",
    "\n",
    "for term, count in termFreqCounter.most_common(freqNum):\n",
    "    print(term + ': ' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b8116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create lists of terms and their corresponding counts for plotting\n",
    "top_terms = [term for term, count in termFreqCounter.most_common(freqNum)]\n",
    "term_counts = [count for term, count in termFreqCounter.most_common(freqNum)]\n",
    "\n",
    "# Create a bar chart using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_terms, term_counts)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel('Terms')\n",
    "plt.ylabel('Term Frequency')\n",
    "plt.title('Top ' + str(freqNum) + ' Most Frequent Terms in Submission Title')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fbba3",
   "metadata": {},
   "source": [
    "### Analyzing Term Frequency in Reddit Submissions Titles and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b7cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine submission titles and comments text\n",
    "texts = [] # \"texts\" is the combination of submission titles and comments text\n",
    "for submission in dSubmissions['submissions']:\n",
    "    texts.append(submission.get('title', ''))\n",
    "    for comment in submission['comments']:\n",
    "        texts.append(comment['text'])\n",
    "\n",
    "# Tokenise, filter stopwords and get convert to lower case\n",
    "tokenized_texts = []\n",
    "for text in texts:\n",
    "    tokens = processText(text, tweetTokeniser, tweetStemmer, lStopwords)\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "for tokens in tokenized_texts:\n",
    "     # All tokens of each text are combined for further topic modelling, LDA.\n",
    "    lPosts_LDA.append(' '.join(tokens)) \n",
    "    termFreqCounter.update(tokens)\n",
    "\n",
    "for term, count in termFreqCounter.most_common(freqNum):\n",
    "    print(term + ': ' + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e90c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of terms and their corresponding counts for plotting\n",
    "top_terms = [term for term, count in termFreqCounter.most_common(freqNum)]\n",
    "term_counts = [count for term, count in termFreqCounter.most_common(freqNum)]\n",
    "\n",
    "# Create a bar chart using matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_terms, term_counts)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel('Terms')\n",
    "plt.ylabel('Term Frequency')\n",
    "plt.title('Top ' + str(freqNum) + ' Most Frequent Terms in Submission Titles and Comments')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf3b9c1",
   "metadata": {},
   "source": [
    "### Token counts before and after pre-processing and cleaning submission title and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419be43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "total_original_tokens = 0\n",
    "total_processed_tokens = 0\n",
    "\n",
    "# Iterate through submissions and comments\n",
    "for submission in dSubmissions['submissions']:\n",
    "    # Count tokens in the original title\n",
    "    original_title = submission.get('title', '')\n",
    "    original_title_tokens = tweetTokeniser.tokenize(original_title)\n",
    "    total_original_tokens += len(original_title_tokens)\n",
    "    \n",
    "    # Process title and count processed tokens\n",
    "    processed_title_tokens = processText(original_title, tweetTokeniser, tweetStemmer, lStopwords)\n",
    "    total_processed_tokens += len(processed_title_tokens)\n",
    "    \n",
    "    # Process and count tokens in comments\n",
    "    for comment in submission['comments']:\n",
    "        comment_text = comment['text']\n",
    "        comment_tokens = tweetTokeniser.tokenize(comment_text)\n",
    "        total_original_tokens += len(comment_tokens)\n",
    "        \n",
    "        processed_comment_tokens = processText(comment_text, tweetTokeniser, tweetStemmer, lStopwords)\n",
    "        total_processed_tokens += len(processed_comment_tokens)\n",
    "\n",
    "print(\"Total Original Tokens:\", total_original_tokens)\n",
    "print(\"Total Processed Tokens:\", total_processed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d3cd0",
   "metadata": {},
   "source": [
    "### Tokens Before and After Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e9e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first submission\n",
    "\n",
    "first_submission = dSubmissions['submissions'][100]  # Get the first submission\n",
    "first_submissionsTitle = first_submission.get('title', '')\n",
    "first_lTokens = processText(text=first_submissionsTitle, tokenizer=tweetTokeniser, stemmer=tweetStemmer, stopwords=lStopwords)\n",
    "\n",
    "# Print original and processed text only for the first submission\n",
    "print(\"Original Text:\", first_submissionsTitle)\n",
    "print(\"Processed Text:\", first_lTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9da54",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of Reddit Posts and Comments\n",
    "### Approach 1: Use counts of positive and negative words to calculate the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb4a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSentiment(lTokens, setPosWords, setNegWords):\n",
    "    \"\"\"\n",
    "    Compute the overall sentiment of the list of tokens in lTokens, using the countWordSentimentAnalysis approach.\n",
    "    \n",
    "    @param lTokens: List of tokens to calculate the overall sentiment for.\n",
    "    @param setPosWords: Set of positive words.\n",
    "    @param setNegWords: Set of negative words.\n",
    "    \n",
    "    @returns Sentiment score for lTokens.\n",
    "    \"\"\"\n",
    "    # count the number of positive words \n",
    "    posNum = len([tok for tok in lTokens if tok in setPosWords])\n",
    "    # count the number of negative words \n",
    "    negNum = len([tok for tok in lTokens if tok in setNegWords])\n",
    "    sentiment = posNum - negNum\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ea578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printColouredTokens(lTokens, setPosWords, setNegWords, sentiment):\n",
    "    \"\"\"\n",
    "    If sentiment is positive, token in red\n",
    "    If sentiment is negative, toekn in blue.\n",
    "    Otherwise no colouring.\n",
    "    \n",
    "    @param lTokens: List of tokens to print and colour.\n",
    "    @param setPosWords: Set of positive words.\n",
    "    @param setNegWords: Set of negative words.\n",
    "    @param sentiment: Sentiment score of list of tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    for token in lTokens:\n",
    "        if token in setPosWords:\n",
    "            print(Fore.RED + token + ', ', end='')\n",
    "        elif token in setNegWords:\n",
    "            print(Fore.BLUE + token + ', ', end='')\n",
    "        else:\n",
    "            print(Style.RESET_ALL + token + ', ', end='')\n",
    "\n",
    "    print(': {}'.format(sentiment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a02118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def countWordSentimentAnalysis(setPosWords, setNegWords, sTweetsFilename, bPrint, redditProcessor):\n",
    "    \"\"\"\n",
    "    Basic sentiment analysis.  Count the number of positive words, count the negative words, overall polarity is the\n",
    "    difference in the two numbers.\n",
    "\n",
    "    @param setPosWords: set of positive sentiment words\n",
    "    @param setNegWords: set of negative sentiment words\n",
    "    @param sFilename: name of input file containing a json formated dump\n",
    "    @param bPrint: whether to print the stream of tokens and sentiment.  Uses colorama to highlight sentiment words.\n",
    "    @param redditProcessor: RedditProcessing object, used to pre-process each tweet.\n",
    "\n",
    "    @returns: list of reddit posts, in the format of [date, sentiment]\n",
    "    \"\"\"\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    neutral_count = 0\n",
    "    \n",
    "    lSentiment = []\n",
    "    # Open file and process reddit submissions and comments, one by one\n",
    "    with open(sTweetsFilename, 'r') as f:\n",
    "        redditDump = json.load(f)\n",
    "        \n",
    "        for submission in redditDump['submissions']:\n",
    "            postText = submission['title']\n",
    "            postDate = submission['created']\n",
    "            \n",
    "            # Pre-process the reddit post text\n",
    "            lTokens = redditProcessor.process(postText)\n",
    "            \n",
    "            # Compute sentiment\n",
    "            sentiment = computeSentiment(lTokens, setPosWords, setNegWords)\n",
    "            \n",
    "            if sentiment > 0:\n",
    "                positive_count += 1\n",
    "            elif sentiment < 0:\n",
    "                negative_count += 1\n",
    "            else:\n",
    "                neutral_count += 1\n",
    "\n",
    "                \n",
    "            # Save date and sentiment of each reddit post (used for time series)\n",
    "            lSentiment.append([pd.to_datetime(postDate, unit='s'), sentiment])\n",
    "\n",
    "            # If print, each token is printed and coloured red if positive word, and blue if negative\n",
    "            if bPrint:\n",
    "                printColouredTokens(lTokens, setPosWords, setNegWords, sentiment)\n",
    "                \n",
    "            # Process the comments\n",
    "            for comment in submission['comments']:\n",
    "                postText = comment['text']\n",
    "                postDate = comment['created']\n",
    "                \n",
    "                # Pre-process the text\n",
    "                lTokens = redditProcessor.process(postText)\n",
    "            \n",
    "                # Compute sentiment\n",
    "                sentiment = computeSentiment(lTokens, setPosWords, setNegWords)\n",
    "                \n",
    "                if sentiment > 0:\n",
    "                    positive_count += 1\n",
    "                elif sentiment < 0:\n",
    "                    negative_count += 1\n",
    "                else:\n",
    "                    neutral_count += 1\n",
    "                    \n",
    "                # Save the date and sentiment of each tweet (used for time series)\n",
    "                lSentiment.append([pd.to_datetime(postDate,unit='s'), sentiment])\n",
    "\n",
    "                # If print, each token is printed and coloured according to positive or negative sentiment\n",
    "                if bPrint:\n",
    "                    printColouredTokens(lTokens, setPosWords, setNegWords, sentiment)\n",
    "                    \n",
    "    print(\"Count Method\")\n",
    "    print(\"Positive Sentiments:\", positive_count)\n",
    "    print(\"Negative Sentiments:\", negative_count)\n",
    "    print(\"Neutral Sentiments:\", neutral_count)\n",
    "    return lSentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caa7752",
   "metadata": {},
   "source": [
    "### Approach 2: Vader based approach to sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66548d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderPrintTokens(lTokens, dSentimentScores):\n",
    "    \"\"\"\n",
    "    Print out the tokens and sentiment score.\n",
    "    \n",
    "    @param lTokens: List of tokens to print and colour.\n",
    "    @dSentimentScores: Dictionary of sentiment from Vader.\n",
    "\n",
    "    \"\"\"\n",
    "    print(*lTokens, sep=', ')\n",
    "    for cat,score in dSentimentScores.items():\n",
    "        print('{0}: {1}, '.format(cat, score), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vaderSentimentAnalysis(sTweetsFilename, bPrint, tweetProcessor):\n",
    "    \"\"\"\n",
    "    Use Vader lexicons instead of a raw positive and negative word count.\n",
    "\n",
    "    @param sTweetsFilename: name of input file containing a json formated tweet dump\n",
    "    @param bPrint: whether to print the stream of tokens and sentiment.  Uses colorama to highlight sentiment words.\n",
    "    @param tweetProcessor: TweetProcessing object, used to pre-process each tweet.\n",
    "\n",
    "    @returns: list of tweets, in the format of [date, sentiment]\n",
    "    \"\"\"\n",
    "\n",
    "    # Setting up vader sentiment analyser, part of nltk\n",
    "    sentAnalyser = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Counting sentiments for Vader analysis\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    neutral_count = 0\n",
    "\n",
    "    lSentiment = []\n",
    "    # Open file and process tweets, one by one\n",
    "    with open(sTweetsFilename, 'r') as f:\n",
    "        redditDump = json.load(f)\n",
    "        \n",
    "        for submission in redditDump['submissions']:\n",
    "            postText = submission['title']\n",
    "            postDate = submission['created']\n",
    "            \n",
    "            #Pre-process the post text\n",
    "            lTokens = redditProcessor.process(postText)\n",
    "\n",
    "            #Computes sentiment scores(polarity score in nltk)\n",
    "            dSentimentScores = sentAnalyser.polarity_scores(\" \".join(lTokens))\n",
    "            \n",
    "            sentiment = dSentimentScores['compound']\n",
    "            if sentiment > 0.05:  #Positive sentiment threshold\n",
    "                positive_count += 1\n",
    "            elif sentiment < -0.05:  #Negative sentiment threshold\n",
    "                negative_count += 1\n",
    "            else:\n",
    "                neutral_count += 1\n",
    "                \n",
    "            #Save date and sentiment of each post (used for time series)\n",
    "            lSentiment.append([pd.to_datetime(postDate, unit='s'), dSentimentScores['compound']])\n",
    "\n",
    "            #If print, print tokens then the sentiment scores\n",
    "            #Cannot use colorama to label each token as there's  no list of positive and negative words\n",
    "            if bPrint:\n",
    "                vaderPrintTokens(lTokens, dSentimentScores)\n",
    "                \n",
    "            #Process the comments\n",
    "            for comment in submission['comments']:\n",
    "                postText = comment['text']\n",
    "                postDate = comment['created']\n",
    "                \n",
    "                #Pre-process the post text\n",
    "                lTokens = redditProcessor.process(postText)\n",
    "\n",
    "                #Computes sentiment scores (polarity score in nltk)\n",
    "                dSentimentScores = sentAnalyser.polarity_scores(\" \".join(lTokens))\n",
    "\n",
    "                #Save the date and sentiment of each post (used for time series)\n",
    "                lSentiment.append([pd.to_datetime(postDate, unit='s'), dSentimentScores['compound']])\n",
    "                \n",
    "                sentiment = dSentimentScores['compound']\n",
    "                if sentiment > 0.05:\n",
    "                    positive_count += 1\n",
    "                elif sentiment < -0.05:\n",
    "                    negative_count += 1\n",
    "                else:\n",
    "                    neutral_count += 1\n",
    "\n",
    "                #If print, print tokens then the sentiment scores. \n",
    "                #Cannot use colorama to label each token as there's  no list of positive and negative words\n",
    "                if bPrint:\n",
    "                    vaderPrintTokens(lTokens, dSentimentScores)\n",
    "                    \n",
    "    print(\"Vader Method\")\n",
    "    print(\"Positive Sentiments:\", positive_count)\n",
    "    print(\"Negative Sentiments:\", negative_count)\n",
    "    print(\"Neutral Sentiments:\", neutral_count)\n",
    "\n",
    "    return lSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f818814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input set of postive words file \n",
    "posWordFile = 'positive-words.txt'\n",
    "#Input set of negative words file\n",
    "negWordFile = 'negative-words.txt'\n",
    "#Input set of reddit posts files in json format\n",
    "redditFile = 'environment.json'\n",
    "#Flag to determine whether to print out tweets and their sentiment\n",
    "flagPrint = True\n",
    "#Specify the approach to take (count or vader)\n",
    "approach = 'count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d12de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call the RedditProcessing python script\n",
    "redditProcessor = RedditProcessing.RedditProcessing(tweetTokeniser, lStopwords)\n",
    "\n",
    "# Load set of positive words\n",
    "lPosWords = []\n",
    "with open(posWordFile, 'r', encoding='utf-8', errors='ignore') as fPos:\n",
    "    for sLine in fPos:\n",
    "        lPosWords.append(sLine.strip())\n",
    "setPosWords = set(lPosWords)\n",
    "\n",
    "# load set of negative words\n",
    "lNegWords = []\n",
    "with codecs.open(negWordFile, 'r', encoding='utf-8', errors='ignore') as fNeg:\n",
    "    for sLine in fNeg:\n",
    "        lNegWords.append(sLine.strip())\n",
    "setNegWords = set(lNegWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09899144",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute the sentiment\n",
    "lSentiment = []\n",
    "if approach == 'count':\n",
    "    lSentiment = countWordSentimentAnalysis(setPosWords, setNegWords, redditFile, flagPrint, redditProcessor)\n",
    "elif approach == 'vader':\n",
    "    lSentiment = vaderSentimentAnalysis(redditFile, flagPrint, redditProcessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the \"Vader Method\" and \"Count Method\" separately\n",
    "vader_sentiment = vaderSentimentAnalysis(redditFile, flagPrint, redditProcessor)\n",
    "count_sentiment = countWordSentimentAnalysis(setPosWords, setNegWords, redditFile, flagPrint, redditProcessor)\n",
    "\n",
    "# Create DataFrames for each sentiment analysis method\n",
    "vader_series = pd.DataFrame(vader_sentiment, columns=['date', 'vader_sentiment'])\n",
    "vader_series.set_index('date', inplace=True)\n",
    "\n",
    "count_series = pd.DataFrame(count_sentiment, columns=['date', 'count_sentiment'])\n",
    "count_series.set_index('date', inplace=True)\n",
    "\n",
    "# Resample sentiment scores on an hourly basis\n",
    "vader_resampled = vader_series.resample('1H').sum()\n",
    "count_resampled = count_series.resample('1H').sum()\n",
    "\n",
    "# Combine the two DataFrames into a single DataFrame\n",
    "combined_series = pd.concat([vader_resampled, count_resampled], axis=1)\n",
    "\n",
    "# Plot the combined DataFrame\n",
    "combined_series.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900ed050",
   "metadata": {},
   "source": [
    "### Text Analysis and Topic Modeling of Reddit Submissions and Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f498df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of topics to discover\n",
    "topicNum = 5\n",
    "# Maximum number of words to display per topic\n",
    "wordNumToDisplay = 15\n",
    "# Number of features/words to used to describe our documents\n",
    "featureNum = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2993d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfVectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=featureNum, stop_words='english')\n",
    "tf = tfVectorizer.fit_transform(lPosts_LDA)\n",
    "# Extract the names of the features \n",
    "tfFeatureNames = tfVectorizer.get_feature_names_out()\n",
    "\n",
    "# Run LDA (see documentation about what the arguments means)\n",
    "ldaModel = LatentDirichletAllocation(n_components =topicNum, max_iter=10, learning_method='online').fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f27185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, featureNames, numTopWords):\n",
    "    \"\"\"\n",
    "    Prints out the most associated words for each feature.\n",
    "\n",
    "    @param model: lda model.\n",
    "    @param featureNames: list of strings, representing the list of features/words.\n",
    "    @param numTopWords: number of words to print per topic.\n",
    "    \"\"\"\n",
    "    # Print out the topic distributions\n",
    "    for topicId, lTopicDist in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topicId+1))\n",
    "        print(\" \".join([featureNames[i] for i in lTopicDist.argsort()[:-numTopWords - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d794bfda",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_topics(ldaModel, tfFeatureNames, wordNumToDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61092e75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "panel = pyLDAvis.lda_model.prepare(ldaModel, tf, tfVectorizer, mds='tsne')\n",
    "pyLDAvis.display(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ae3d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayWordcloud(model, featureNames):\n",
    "    \"\"\"\n",
    "    Displays the word cloud of the topic distributions, stored in model.\n",
    "\n",
    "    @param model: lda model.\n",
    "    @param featureNames: list of strings, representing the list of features/words.\n",
    "    \"\"\"\n",
    "\n",
    "    # This normalises each row/topic to sum to one, use this normalisedComponents to display your wordclouds\n",
    "    normalisedComponents = model.components_ / model.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    topicNum = len(model.components_)\n",
    "    # Number of wordclouds for each row\n",
    "    plotColNum = 3\n",
    "    # Number of wordclouds for each column\n",
    "    plotRowNum = int(math.ceil(topicNum / plotColNum))\n",
    "\n",
    "    for topicId, lTopicDist in enumerate(normalisedComponents):\n",
    "        lWordProb = {featureNames[i] : wordProb for i,wordProb in enumerate(lTopicDist)}\n",
    "        wordcloud = WordCloud(background_color='black')\n",
    "        wordcloud.fit_words(frequencies=lWordProb)\n",
    "        plt.subplot(plotRowNum, plotColNum, topicId+1)\n",
    "        plt.title('Topic %d:' % (topicId+1))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a244c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display wordcloud\n",
    "displayWordcloud(ldaModel, tfFeatureNames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
